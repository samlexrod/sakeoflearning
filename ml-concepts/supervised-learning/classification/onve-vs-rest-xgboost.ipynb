{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# One Versus Rest & One Versis One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Iris Dataset Example\n",
    "The iris dataset is a classic and very easy multi-class classification dataset.\n",
    "This dataset consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) \n",
    "petal and sepal length, stored in a 150x4 numpy.ndarray.\n",
    "\n",
    "It helps to understand the basic steps of training a multi-class classification model in \n",
    "a very simple and easy-to-understand dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# Wrap the XGBoost model with OneVsRestClassifier\n",
    "ovr_model = OneVsRestClassifier(xgb_model)\n",
    "\n",
    "# Train the OneVsRest model\n",
    "ovr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ovr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"One-vs-Rest with XGBoost Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Customer Segmentation Example\n",
    "\n",
    "By using segmented customer data, we can achieve better model performance by training separate models for each segment.\n",
    "In this approach, we are generating synthetic customer data with random values for the features and segmenting the \n",
    "customers into three segments: 'High Value', 'Medium Value', and 'Low Value'.\n",
    "\n",
    "However, in a real-world scenario, you would use actual customer data and segment the customers based on their behavior,\n",
    "such as purchase history, spending patterns, etc. This means that you will not achieve the same level of accuracy as in\n",
    "this synthetic example, but the concept of segmenting customers and training separate models for each segment still applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def segmented_anual_spending(segment):\n",
    "    \"\"\"\n",
    "    Generate a random annual spending based on the segment.\n",
    "    \"\"\"\n",
    "    if segment == 'High Value':\n",
    "        return np.random.uniform(10000, 20000)\n",
    "    elif segment == 'Medium Value':\n",
    "        return np.random.uniform(5000, 10000)\n",
    "    else:\n",
    "        return np.random.uniform(1000, 5000)\n",
    "    \n",
    "def segmented_number_of_purchases(segment):\n",
    "    \"\"\"\n",
    "    Generate a random number of purchases based on the segment.\n",
    "    \"\"\"\n",
    "    if segment == 'High Value':\n",
    "        return np.random.randint(50, 100)\n",
    "    elif segment == 'Medium Value':\n",
    "        return np.random.randint(20, 50)\n",
    "    else:\n",
    "        return np.random.randint(1, 20)        \n",
    "\n",
    "def generate_customer_data(num_samples):\n",
    "    \"\"\"\n",
    "    Generate synthetic customer data with random values for the features.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "\n",
    "    # Generate random segments for the customers\n",
    "    segments = np.random.choice(['High Value', 'Medium Value', 'Low Value'], num_samples)\n",
    "\n",
    "    customer_ids = np.arange(1, num_samples + 1)\n",
    "    annual_spending = np.array([segmented_anual_spending(segment) for segment in segments])\n",
    "    number_of_purchases = np.array([segmented_number_of_purchases(segment) for segment in segments])\n",
    "    avg_purchase_value = annual_spending / number_of_purchases\n",
    "    ages = np.random.randint(18, 70, num_samples)\n",
    "    tenure = np.random.randint(1, 20, num_samples)\n",
    "    \n",
    "\n",
    "    data = {\n",
    "        'CustomerID': customer_ids,\n",
    "        'AnnualSpending': annual_spending,\n",
    "        'NumberOfPurchases': number_of_purchases,\n",
    "        'AvgPurchaseValue': avg_purchase_value,\n",
    "        'Age': ages,\n",
    "        'Tenure': tenure,\n",
    "        'Segment': segments\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate a larger dataset\n",
    "num_samples = 1000000  # Adjust the number of samples as needed\n",
    "df = generate_customer_data(num_samples)\n",
    "\n",
    "# Drop CustomerID as it is not a useful feature for the model\n",
    "df = df.drop('CustomerID', axis=1)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['Segment'] = label_encoder.fit_transform(df['Segment'])\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop('Segment', axis=1)\n",
    "y = df['Segment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# Decode the predicted labels back to the original categories\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "print(\"Predicted Segments:\", y_pred_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Summary of Predictions\n",
    "In this summary, we can first validate the number of samples in the testing set and the number of predicted segments.\n",
    "Next, we can validate the mapping of the labels to the original categories using the label encoder.\n",
    "Finally, we can display a sample of the testing set with the actual segment and the predicted segment to visually validate the predictions.\n",
    "\n",
    "We can use the following code to summarize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View counts to validate\n",
    "print(f\"\"\"\n",
    "TESTING SET COUNT: {X_test.shape[0]:,.0f}\n",
    "PREDICTED SEGMENT COUNTS: {len(y_pred_labels):,.0f}\n",
    "\"\"\")\n",
    "\n",
    "# View the mapping of labels to validate\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Encoder Mapping:\", label_mapping)\n",
    "\n",
    "predicted_test_df = pd.concat([X_test, y_test.to_frame()], axis=1)\n",
    "predicted_test_df[\"prediction\"] = y_pred_labels\n",
    "\n",
    "predicted_test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Example of one customer record\n",
    "\n",
    "In this example, we have a customer with the following features:\n",
    "- Annual Spending: $12,000\n",
    "- Number of Purchases: 75\n",
    "- Average Purchase Value: $160\n",
    "- Age: 35\n",
    "- Tenure: 15 years\n",
    "\n",
    "We can use the trained model to predict the segment for this customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customer_data = {\n",
    "    'AnnualSpending': [12000],\n",
    "    'NumberOfPurchases': [75],\n",
    "    'AvgPurchaseValue': [160],\n",
    "    'Age': [35],\n",
    "    'Tenure': [15]\n",
    "}\n",
    "\n",
    "new_customer_df = pd.DataFrame(new_customer_data)\n",
    "\n",
    "# Make predictions for the new customer data\n",
    "new_customer_pred = model.predict(new_customer_df)\n",
    "new_customer_pred_label = label_encoder.inverse_transform(new_customer_pred)\n",
    "print(\"Predicted Segment for New Customer:\", new_customer_pred_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# END OF NOTEOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
