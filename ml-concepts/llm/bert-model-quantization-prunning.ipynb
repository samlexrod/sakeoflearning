{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization and Pruning with TensorFlow Model Optimization Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "For this example, we will use the AG News dataset. The dataset contains 120,000 training samples and 7,600 test samples. Each sample consists of a title and a description of a news article, and a label that classifies the article into one of four categories: World, Sports, Business, and Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# Preview the dataset\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this process we will use the `BertTokenizer` from the `transformers` library to tokenize the text data. We will also apply padding and truncation to ensure that all sequences have the same length. Finally, we will format the datasets for PyTorch. \n",
    "\n",
    "> Note: Be careful with truncation, as it may remove important information from the text data. Also be careful with padding, as it may introduce noise into the data. For example, if you are working with text data that has a lot of padding, you may want to consider using a smaller batch size to reduce the amount of padding in the data.\n",
    "\n",
    "---\n",
    "**Truncation** \n",
    "It is the process of removing tokens from the beginning or end of a sequence to make it fit within a certain length. This is useful when working with sequences that are longer than the maximum length allowed by the model.\n",
    "\n",
    "In this example the max_length is 5 and the padding token is `[PAD]`.\n",
    "\n",
    "Text: \"This is a test sentence with more than 5 words.\"\n",
    "\n",
    "Tokenized: `['This', 'is', 'a', 'test', 'sentence', 'with', 'more', 'than', '5', 'words', '.']`\n",
    "\n",
    "Truncated: `['This', 'is', 'a', 'test', 'sentence']`\n",
    "\n",
    "---\n",
    "**Padding** is the process of adding tokens to the end of a sequence to make it fit within a certain length. This is useful when working with sequences that are shorter than the maximum length allowed by the model.\n",
    "\n",
    "In this example the max_length is 20 and the padding token is `[PAD]`.\n",
    "sentence: \"This is a sentence with less than 20 words.\"\n",
    "\n",
    "Tokenized: `['This', 'is', 'a', 'sentence', 'with', 'less', 'than', '20', 'words', '.']`\n",
    "\n",
    "Padded: `['This', 'is', 'a', 'sentence', 'with', 'less', 'than', '20', 'words', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenizer\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "We will use the `BertForSequenceClassification` model from the `transformers` library. This model is a pre-trained BERT model that has been fine-tuned for sequence classification tasks. We will load the pre-trained model with the `from_pretrained` method and specify the number of labels for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BERT for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pruining\n",
    "\n",
    "Pruning is a technique used to reduce the size of a neural network by removing unimportant weights. This can help reduce the computational cost of running the model and make it more efficient. In this example, we will use the `prune` method from the `transformers` library to prune the pre-trained BERT model.\n",
    "\n",
    "Pros:\n",
    "- Reduces the size of the model\n",
    "- Reduces the computational cost of running the model\n",
    "- Can improve the efficiency of the model\n",
    "\n",
    "Cons:\n",
    "- May reduce the accuracy of the model\n",
    "- May require re-training the model after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "# Prune the linear layers in the attention mechanism\n",
    "for name, module in model.bert.encoder.layer[0].attention.self.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)  # Prune 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fine-Tune the Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to predictions\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,  # Pruned model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the pruned model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"Pruned and Fine-Tuned Model Accuracy: {results['eval_accuracy']:.2f}\")\n",
    "\n",
    "trainer.save_model(\"./models/pruned_fine_tuned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pruned model\n",
    "model = BertForSequenceClassification.from_pretrained(\"./models/pruned_fine_tuned_model.pth\")\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Evaluate the quantized model\n",
    "quantized_results = trainer.evaluate(quantized_model)\n",
    "\n",
    "print(f\"Quantized Model Accuracy: {quantized_results['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-aware Training (QAT)\n",
    "\n",
    "QAT is a technique that trains a model with quantization in mind. It simulates the effects of quantization during training, allowing the model to learn to be more robust to the quantization process. This can lead to better performance than when the model is quantized post-training.\n",
    "\n",
    "Under the hood, this method uses the QuantizationAwareTraining class from the TensorFlow Model Optimization Toolkit to perform quantization-aware training. During training, it simulates the effects of converting 32-bit floating-point weights and activations into 8-bit integers, allowing the model to learn to compensate for quantization-induced errors. At inference time, the quantized 8-bit representation is typically used, which reduces the size of the model and improves computational efficiency.\n",
    "\n",
    "Pros:\n",
    "- Can improve the performance of the quantized model\n",
    "- Can reduce the size of the model\n",
    "- Can reduce the computational cost of running the model\n",
    "\n",
    "Cons:\n",
    "- May require additional training time\n",
    "- May require additional computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import torch\n",
    "\n",
    "class QATBERT(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(QATBERT, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.model = model\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        input_ids = self.quant(input_ids)\n",
    "        output = self.model(input_ids, attention_mask, token_type_ids, labels)\n",
    "        return self.dequant(output.logits)\n",
    "\n",
    "qat_model = QATBERT(model)\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print(\"QAT model prepared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fine-tune the QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=qat_model.model,  # QAT-enabled model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.quantization.convert(qat_model, inplace=True)\n",
    "print(\"Fully quantized model ready for inference.\")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"Quantized and Fine-Tuned Model Accuracy: {results['eval_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT University",
   "language": "python",
   "name": "gptu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
