{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threaded Fast Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Retrieve the GEMA_TOKEN from environment variables\n",
    "GEMA_TOKEN = os.getenv(\"GEMMA_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", use_auth_token=GEMA_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32,\n",
    "    use_auth_token=GEMA_TOKEN,\n",
    ")\n",
    "\n",
    "input_text = \"Write me a story about a dragon.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=300)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Check if the tokenizer is fast\n",
    "print(f\"Tokenizer is fast: {tokenizer.is_fast}\")\n",
    "\n",
    "# Function to tokenize\n",
    "def tokenize(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Test multi-threading\n",
    "texts = [\"This is a test sentence.\"] * 10000000\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "def batchify(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# Tokenize in batches\n",
    "def process_in_parallel(texts, batch_size, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        return list(executor.map(tokenize, batchify(texts, batch_size)))\n",
    "\n",
    "\n",
    "# Tokenize in parallel\n",
    "process_in_parallel(texts, 64, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-threading Fast Tokenization & LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from itertools import islice\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to tokenize a batch of texts and run inference\n",
    "def tokenize_and_infer(batch):\n",
    "    # Tokenize the batch\n",
    "    tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits\n",
    "    # Return predictions\n",
    "    return logits.argmax(dim=1).tolist()  # Predicted classes\n",
    "\n",
    "# Batchify texts\n",
    "def batchify(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# Process texts in parallel (tokenization + inference)\n",
    "def process_in_parallel(texts, batch_size, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:  # Adjust threads as needed\n",
    "        return list(executor.map(tokenize_and_infer, batchify(texts, batch_size)))\n",
    "\n",
    "# Example texts\n",
    "texts = [\"This is a test sentence.\"] * 100000\n",
    "\n",
    "# Process texts\n",
    "batch_size = 64\n",
    "max_workers = 14\n",
    "results = process_in_parallel(texts, batch_size, max_workers=max_workers)\n",
    "\n",
    "# Display results\n",
    "print(results[:10])  # Print the first 10 predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Results As Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from itertools import islice\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to tokenize a batch of texts and run inference\n",
    "def tokenize_and_infer(batch):\n",
    "    # Tokenize the batch\n",
    "    tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits\n",
    "    # Return predictions\n",
    "    return logits.argmax(dim=1).tolist()  # Predicted classes\n",
    "\n",
    "# Batchify texts\n",
    "def batchify(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# Process texts in parallel (tokenization + inference)\n",
    "def process_in_parallel(texts, batch_size, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:  # Adjust threads as needed\n",
    "        # Submit tasks and map batches to futures\n",
    "        futures = {executor.submit(tokenize_and_infer, batch): batch for batch in batchify(texts, batch_size)}\n",
    "        \n",
    "        # Process futures as they complete\n",
    "        results = []\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.extend(result)  # Collect the result\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {futures[future]}, {e}\")\n",
    "        return results\n",
    "\n",
    "# Process texts\n",
    "batch_size = 64\n",
    "max_workers = 14\n",
    "results = process_in_parallel(texts, batch_size, max_workers=max_workers)\n",
    "\n",
    "# Display results\n",
    "print(results[:10])  # Print the first 10 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding CPU Usage Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from itertools import islice\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Global list to collect CPU usages\n",
    "cpu_usages = []\n",
    "\n",
    "# Function to tokenize a batch of texts and monitor CPU usage\n",
    "def tokenize_and_infer(batch):\n",
    "    global cpu_usages\n",
    "    process = psutil.Process()  # Current process\n",
    "\n",
    "    # Start monitoring CPU usage with a time gap\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=0.1)  # Short interval for CPU measurement\n",
    "\n",
    "    # Tokenize the batch\n",
    "    tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Stop monitoring CPU usage\n",
    "    elapsed_time = time.time() - start_time\n",
    "    end_cpu = process.cpu_percent(interval=0.1)  # Another snapshot after processing\n",
    "\n",
    "    # Calculate average CPU usage during the interval\n",
    "    avg_cpu_usage = (start_cpu + end_cpu) / 2\n",
    "    cpu_usages.append(avg_cpu_usage)  # Collect CPU usage\n",
    "\n",
    "    print(f\"\\rTokenization Avg CPU Usage: {avg_cpu_usage:.2f}%, Elapsed Time: {elapsed_time:.4f}s\", end='', flush=True)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits\n",
    "\n",
    "    # Return predictions\n",
    "    return logits.argmax(dim=1).tolist()  # Predicted classes\n",
    "\n",
    "# Batchify texts\n",
    "def batchify(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# Process texts in parallel (tokenization + inference)\n",
    "def process_in_parallel(texts, batch_size, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks and map batches to futures\n",
    "        futures = {executor.submit(tokenize_and_infer, batch): batch for batch in batchify(texts, batch_size)}\n",
    "\n",
    "        # Process futures as they complete\n",
    "        results = []\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.extend(result)  # Collect the result\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {futures[future]}, {e}\")\n",
    "        return results\n",
    "\n",
    "# Example texts\n",
    "texts = [\"This is a test sentence.\"] * 100000\n",
    "\n",
    "# Process texts\n",
    "batch_size = 64\n",
    "max_workers = 4\n",
    "results = process_in_parallel(texts, batch_size, max_workers=max_workers)\n",
    "\n",
    "# Calculate average CPU usage\n",
    "average_cpu_usage = sum(cpu_usages) / len(cpu_usages) if cpu_usages else 0\n",
    "print(f\"\\nAverage CPU Usage for Tokenization: {average_cpu_usage:.2f}%\")\n",
    "\n",
    "# Display results\n",
    "print(results[:10])  # Print the first 10 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT University",
   "language": "python",
   "name": "gptu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
