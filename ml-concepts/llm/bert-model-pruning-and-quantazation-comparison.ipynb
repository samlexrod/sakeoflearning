{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, pipeline\n",
    "from torch.nn.utils import prune\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "original_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Load a test dataset (e.g., IMDB small subset)\n",
    "dataset = load_dataset(\"imdb\", split=\"test[:50%]\")  # Using a small portion for demonstration\n",
    "texts = dataset[\"text\"]\n",
    "labels = dataset[\"label\"]\n",
    "\n",
    "# Check for cpu type if arm or x86\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def get_cpu_type():\n",
    "    # Check if the CPU is ARM or x86\n",
    "    if \"aarch64\" in os.uname().machine:\n",
    "        return \"arm\"\n",
    "    else:\n",
    "        return \"x86\"\n",
    "    \n",
    "cpu_type = get_cpu_type()\n",
    "if cpu_type == \"arm\":\n",
    "    # Enable quantization backend\n",
    "    torch.backends.quantized.engine = \"qnnpack\"  # Use 'fbgemm' for x86 CPUs\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(model, texts, labels, tokenizer):\n",
    "    # Create a pipeline for sentiment analysis\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, truncation=True, padding=True)\n",
    "    \n",
    "    # Accuracy calculation\n",
    "    correct = 0\n",
    "    total_time = 0\n",
    "    for text, label in zip(texts, labels):\n",
    "        start_time = time.time()\n",
    "        prediction = classifier(text)[0][\"label\"]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Convert prediction to binary label\n",
    "        pred_label = 1 if prediction == \"POSITIVE\" else 0\n",
    "        if pred_label == label:\n",
    "            correct += 1\n",
    "        \n",
    "        total_time += (end_time - start_time)\n",
    "    \n",
    "    accuracy = correct / len(texts)\n",
    "    avg_time = total_time / len(texts)\n",
    "    return accuracy, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the original model\n",
    "original_model.eval()\n",
    "print(\"Evaluating Original Model...\")\n",
    "orig_acc, orig_time = evaluate_model(original_model, texts, labels, tokenizer)\n",
    "print(f\"Original Model Accuracy: {orig_acc:.2f}, Average Inference Time: {orig_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the model\n",
    "print(\"\\nApplying Pruning...\")\n",
    "pruned_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "for name, module in pruned_model.bert.encoder.layer[0].attention.self.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)  # Prune 20% weights\n",
    "pruned_model.eval()\n",
    "\n",
    "print(\"Evaluating Pruned Model...\")\n",
    "pruned_acc, pruned_time = evaluate_model(pruned_model, texts, labels, tokenizer)\n",
    "print(f\"Pruned Model Accuracy: {pruned_acc:.2f}, Average Inference Time: {pruned_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize the model\n",
    "print(\"\\nApplying Quantization...\")\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    original_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "quantized_model.eval()\n",
    "\n",
    "print(\"Evaluating Quantized Model...\")\n",
    "quant_acc, quant_time = evaluate_model(quantized_model, texts, labels, tokenizer)\n",
    "print(f\"Quantized Model Accuracy: {quant_acc:.2f}, Average Inference Time: {quant_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT University",
   "language": "python",
   "name": "gptu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
