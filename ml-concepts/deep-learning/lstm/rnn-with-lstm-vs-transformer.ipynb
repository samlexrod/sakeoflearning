{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with LSTM VS Transformer with self-attention\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "RNN with LSTM: The LSTM processes sequences one step at a time, maintaining a hidden state that captures information about previous steps.\n",
    "Transformer: The Transformer processes entire sequences simultaneously using self-attention mechanisms, making it highly efficient for long-range dependencies and parallelizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "text = \"This is a simple example of text generation using LSTM. LSTM models are useful for sequence prediction.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences using the tokenized text\n",
    "input_sequences = []\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Create predictors and label\n",
    "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"This is a\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "    output_word = tokenizer.index_word[predicted[0]]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the Transformerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The process of converting text data into numerical data is called tokenization. Tokenization is a crucial step in natural language processing (NLP) tasks, as most machine learning algorithms require numerical data as input. In the context of text generation, tokenization is used to convert words into numerical tokens that can be used as input to a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Sample text data\n",
    "text = \"This is a simple example of text generation using Transformer. Transformers are powerful models for sequence prediction.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "pretty_word_index = str(tokenizer.word_index).replace(\", \", \",\\n\")\n",
    "print(f\"\"\"\n",
    "Tokenizer Explanation:\n",
    "- The tokenizer is used to convert text data into sequences of tokens.\n",
    "- The `fit_on_texts` method is used to fit the tokenizer on the text data.\n",
    "- The `word_index` attribute of the tokenizer contains a dictionary mapping words to their indices.\n",
    "- `word_index` + 1 is done to account for the padding token.\n",
    "\n",
    "Total Words: {total_words}\n",
    "Word Index: \n",
    "{pretty_word_index}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Tokenized Sequence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create input sequences using the tokenized text\n",
    "input_sequences = []\n",
    "\n",
    "print(\"Splitting text into lines and creating n-grams...\")\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    print(f\"\"\"\n",
    "    -> Line: {line}\n",
    "    -> Token List: {token_list}\n",
    "    \"\"\".strip())\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        print(f\"\\t-> N-gram Sequence: {n_gram_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_input_sequences = str(input_sequences).replace(\"],\", \"],\\n\")\n",
    "print(f\"\"\"\n",
    "Explanation:\n",
    "- The text is split into lines.\n",
    "- Each line is tokenized using the tokenizer.\n",
    "- For each token list, n-gram sequences are created by taking the first i tokens.\n",
    "- The n-gram sequences are added to the input sequences.\n",
    "\n",
    "Input Sequences:\n",
    "{pretty_input_sequences}   \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "pretty_padded_input_sequences = str(input_sequences).replace(\"],\", \"],\\n\")\n",
    "print(f\"\"\"\n",
    "Explanation:\n",
    "- The input sequences are padded to the maximum sequence length.\n",
    "- The maximum sequence length is the length of the longest n-gram sequence.\n",
    "- The input sequences are converted to a numpy array.\n",
    "- The sequences are padded with zeros at the beginning.\n",
    "\n",
    "Padded Input Sequences: \n",
    "{pretty_padded_input_sequences}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Create Predictors and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictors and label\n",
    "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "\n",
    "print(f\"\"\"\n",
    "Explanation:\n",
    "- The predictors are the input sequences without the last token.\n",
    "- The label is the last token in the input sequences.\n",
    "- The label is the last token because we are predicting the next token.\n",
    "\"\"\")\n",
    "print(f\"\"\"\n",
    "Predictors:\n",
    "{X}\n",
    "\"\"\".strip())\n",
    "print(f\"\"\"\n",
    "Label:\n",
    "{y}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Hot Encoding the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "print(f\"\"\"\n",
    "Explanation:\n",
    "- The number of classes is the total number of unique words in the text.\n",
    "- The label is represented as a one-hot encoded vector.\n",
    "    \n",
    "One-hot Encoded Label:\n",
    "{hot_y}\n",
    "\"\"\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(f\"\"\"\n",
    "Visualization Explanation:\n",
    "We are converting the one-hot encoded label to a DataFrame for better visualization.\n",
    "So we can understand the one-hot encoded label better.\n",
    "\"\"\")\n",
    "\n",
    "word_idx_columns = [\"%s_%s\" % (word, idx) for word, idx in tokenizer.word_index.items()]\n",
    "pd.DataFrame(hot_y, columns=[\"padding_index_0\"]+ word_idx_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Transformer model\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(ff_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "maxlen = max_sequence_len - 1\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "outputs = Dense(total_words, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"This is a\"\n",
    "next_words = 10\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "    output_word = tokenizer.index_word[predicted[0]]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
