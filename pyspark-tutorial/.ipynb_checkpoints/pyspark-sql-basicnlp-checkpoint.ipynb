{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import split, explode, regexp_replace, lower, regexp_extract\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id as identity, col, length\n",
    "from pyspark.sql.functions import lag, lead\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkNlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Twitter Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Twitter Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative documents\n",
    "pos_tw = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tw = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# joining documents\n",
    "document = [pos_tw] + [neg_tw]\n",
    "\n",
    "# list to dataframe\n",
    "df = pd.DataFrame(document[0]).append(pd.DataFrame(document[1])).rename(columns={0:'text', 1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df_rdd = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp Table Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView('SqlNlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |\n",
      "|@97sides CONGRATS :)                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.select('text').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Invalid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                text|label|          clean_text|\n",
      "+--------------------+-----+--------------------+\n",
      "|#FollowFriday @Fr...|  pos|followfriday fran...|\n",
      "|@Lamb2ja Hey Jame...|  pos|lambja hey james ...|\n",
      "|@DespiteOfficial ...|  pos|despiteofficial w...|\n",
      "|@97sides CONGRATS :)|  pos|     sides congrats |\n",
      "|yeaaaah yippppy!!...|  pos|yeaaaah yippppy  ...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_rdd.withColumn('clean_text', lower(regexp_replace('text', '[^a-zA-Z ]', '')))\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|words                                                                                                                               |label|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[followfriday, franceinte, pkuchly, milipolparis, for, being, top, engaged, members, in, my, community, this, week, ]               |pos  |\n",
      "|[lambja, hey, james, how, odd, , please, call, our, contact, centre, on, , and, we, will, be, able, to, assist, you, , many, thanks]|pos  |\n",
      "|[despiteofficial, we, had, a, listen, last, night, , as, you, bleed, is, an, amazing, track, when, are, you, in, scotland]          |pos  |\n",
      "|[sides, congrats, ]                                                                                                                 |pos  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split = df_clean.select(split('clean_text', ' ').alias('words'), 'label')\n",
    "df_split.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding the Words into Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         for|  pos|\n",
      "|       being|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|          in|  pos|\n",
      "|          my|  pos|\n",
      "|   community|  pos|\n",
      "|        this|  pos|\n",
      "|        week|  pos|\n",
      "|            |  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         how|  pos|\n",
      "|         odd|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_split.select(explode('words').alias('word'), 'label')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         for|  pos|\n",
      "|       being|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|          in|  pos|\n",
      "|          my|  pos|\n",
      "|   community|  pos|\n",
      "|        this|  pos|\n",
      "|        week|  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         how|  pos|\n",
      "|         odd|  pos|\n",
      "|      please|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.filter(col('word')!='')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|   community|  pos|\n",
      "|        week|  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         odd|  pos|\n",
      "|      please|  pos|\n",
      "|        call|  pos|\n",
      "|     contact|  pos|\n",
      "|      centre|  pos|\n",
      "|        able|  pos|\n",
      "|      assist|  pos|\n",
      "|        many|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "df_exp = df_exp.filter(~col('word').isin(stopset))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+\n",
      "|        word|label| id|\n",
      "+------------+-----+---+\n",
      "|followfriday|  pos|  0|\n",
      "|  franceinte|  pos|  1|\n",
      "|     pkuchly|  pos|  2|\n",
      "|milipolparis|  pos|  3|\n",
      "|         top|  pos|  4|\n",
      "|     engaged|  pos|  5|\n",
      "|     members|  pos|  6|\n",
      "|   community|  pos|  7|\n",
      "|        week|  pos|  8|\n",
      "|      lambja|  pos|  9|\n",
      "|         hey|  pos| 10|\n",
      "|       james|  pos| 11|\n",
      "|         odd|  pos| 12|\n",
      "|      please|  pos| 13|\n",
      "|        call|  pos| 14|\n",
      "|     contact|  pos| 15|\n",
      "|      centre|  pos| 16|\n",
      "|        able|  pos| 17|\n",
      "|      assist|  pos| 18|\n",
      "|        many|  pos| 19|\n",
      "+------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.withColumn('id', identity())\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.createOrReplaceTempView('WindowTutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    LAG(word, 1) OVER(ORDER BY id) AS w1,\n",
    "    word,\n",
    "    LEAD(word, 1) OVER(ORDER BY id) AS w2\n",
    "FROM WindowTutorial\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy('id')\n",
    "df_exp.select(\n",
    "    'id',\n",
    "    lag('word', 1).over(w).alias('w1'),\n",
    "    'word',\n",
    "    lead('word', 1).over(w).alias('w2')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart = df_exp.repartition('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repart.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window as Subquery: Most common 3-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|label|              w1|              w2|              w3|              w4|phrase_count|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|  pos|             amp|httptcorcvcyyoiq|          follow|               u|          62|\n",
      "|  pos|httptcorcvcyyoiq|          follow|               u|            back|          62|\n",
      "|  pos|          follow|         jnlazts|             amp|httptcorcvcyyoiq|          62|\n",
      "|  pos|         jnlazts|             amp|httptcorcvcyyoiq|          follow|          62|\n",
      "|  neg|        followed|          thanks| andjustinbieber|          please|          51|\n",
      "|  neg|          thanks| andjustinbieber|          please|        followed|          51|\n",
      "|  pos|              hi|             bam|   barsandmelody|          follow|          44|\n",
      "|  pos|           horan|           loves|             lot|             see|          44|\n",
      "|  pos|             see|          warsaw|              lt|            love|          44|\n",
      "|  pos|             bam|   barsandmelody|          follow|      bestfriend|          44|\n",
      "|  pos|   barsandmelody|          follow|      bestfriend|           horan|          44|\n",
      "|  pos|           loves|             lot|             see|          warsaw|          44|\n",
      "|  pos|           stats|             day|         arrived|             new|          44|\n",
      "|  pos|      bestfriend|           horan|           loves|             lot|          44|\n",
      "|  pos|             lot|             see|          warsaw|              lt|          44|\n",
      "|  pos|          follow|      bestfriend|           horan|           loves|          44|\n",
      "|  pos|          warsaw|              lt|            love|              lt|          44|\n",
      "|  pos|         arrived|             new|        follower|     unfollowers|          44|\n",
      "|  pos|             new|        follower|     unfollowers|             via|          44|\n",
      "|  pos|              lt|            love|              lt|               x|          43|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "GROUP BY label, w1, w2, w3, w4\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+--------+\n",
      "|          w1|           w2|                  w3|      w4|\n",
      "+------------+-------------+--------------------+--------+\n",
      "|zzzzzzplease|         dont|                 let|     sun|\n",
      "|        zzzz|       missed|                stop|    take|\n",
      "|   zzzterror|        didnt|                read|donation|\n",
      "|         zzz|      careful|            actually|   black|\n",
      "|         zzz|           xx|       physiotherapy|  friday|\n",
      "|          zz|      airport|            straight|    away|\n",
      "|     zysuzyy|       thanks|                  zy|    lets|\n",
      "|    zynovftw|  mrprowestie|         twoscotsmen|  simple|\n",
      "|          zy|         lets|              friend|     yaa|\n",
      "|     zxwlfxz|           hi|              adrian|  parcel|\n",
      "|    zupiapre|unfortunately|                 yes|    eveh|\n",
      "|       zumba|    somewhere|                else|  please|\n",
      "|   zulbayarb|         masa|nowadayshttptcoif...| brainer|\n",
      "|     zozeebo|       honest|                miss|   dubai|\n",
      "|     zozeebo|    tanyaburr|              please|    tell|\n",
      "|    zoyaashk|         true|            managers|     job|\n",
      "|    zoullgab|       frozen|               bagus|   emang|\n",
      "|   zouiriaii|         good|                luck|      ur|\n",
      "|zorroreturms|        cfvuf|                hope|  helped|\n",
      "|      zoradb|       texted|               dying|   lurgy|\n",
      "+------------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4\n",
    "FROM (\n",
    "    SELECT\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "ORDER BY w1 DESC, w2, w3, w4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|label|      w1|     w2|             w3|              w4|phrase_count|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|  neg|followed| thanks|andjustinbieber|          please|          51|\n",
      "|  pos|  follow|jnlazts|            amp|httptcorcvcyyoiq|          62|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH subquery_cte AS (\n",
    "    SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "    FROM (\n",
    "        SELECT\n",
    "            label,\n",
    "            word AS w1,\n",
    "            LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "            LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "            LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "        FROM WindowTutorial\n",
    "    )\n",
    "    GROUP BY label, w1, w2, w3, w4\n",
    ")\n",
    "SELECT label, w1, w2, w3, w4, phrase_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        ROW_NUMBER() OVER(PARTITION BY label ORDER BY phrase_count DESC) AS row,\n",
    "        w1, w2, w3, w4, phrase_count\n",
    "    FROM subquery_cte\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY label ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
