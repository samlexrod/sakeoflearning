{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import split, explode, regexp_replace, lower, regexp_extract\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id as identity, col, length\n",
    "from pyspark.sql.functions import lag, lead, udf\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkNlp').getOrCreate()\n",
    "webbrowser.open('http://localhost:4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Twitter Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Twitter Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative documents\n",
    "pos_tw = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tw = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# joining documents\n",
    "document = [pos_tw] + [neg_tw]\n",
    "\n",
    "# list to dataframe\n",
    "df = pd.DataFrame(document[0]).append(pd.DataFrame(document[1])).rename(columns={0:'text', 1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df_rdd = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                          |label|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |pos  |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|pos  |\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |pos  |\n",
      "|@97sides CONGRATS :)                                                                                                          |pos  |\n",
      "|yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                    |pos  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creates a job\n",
    "df_rdd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp Table Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView('SqlNlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='sqlnlp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   sqlnlp|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |\n",
      "|@97sides CONGRATS :)                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.select('text').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_test = \"#FollowFriday this is not a sentence #that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday', '#that']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\B#\\w+', hashtag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Invalid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |label|clean_text                                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |pos  |#followfriday @franceinte @pkuchly @milipolparis for being top engaged members in my community this week    |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|pos  |@lambja hey james how odd  please call our contact centre on  and we will be able to assist you  many thanks|\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |pos  |@despiteofficial we had a listen last night  as you bleed is an amazing track when are you in scotland      |\n",
      "|@97sides CONGRATS :)                                                                                                          |pos  |@sides congrats                                                                                             |\n",
      "|yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                    |pos  |yeaaaah yippppy  my accnt verified rqst has succeed got a blue tick mark on my fb profile  in  days         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_rdd.withColumn('clean_text', lower(regexp_replace('text', '[^a-zA-Z#@ ]', '')))\n",
    "df_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|words                                                                                                                                |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[#followfriday, @franceinte, @pkuchly, @milipolparis, for, being, top, engaged, members, in, my, community, this, week, ]            |pos  |\n",
      "|[@lambja, hey, james, how, odd, , please, call, our, contact, centre, on, , and, we, will, be, able, to, assist, you, , many, thanks]|pos  |\n",
      "|[@despiteofficial, we, had, a, listen, last, night, , as, you, bleed, is, an, amazing, track, when, are, you, in, scotland]          |pos  |\n",
      "|[@sides, congrats, ]                                                                                                                 |pos  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split = df_clean.select(split('clean_text', ' ').alias('words'), 'label')\n",
    "df_split.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Hash and User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cotains_hash_user = udf(lambda row: any([any([i in x for i in ['#', '@']]) for x in row]))\n",
    "udf_contains_hash_only = udf(lambda row: any(['#' in x for x in row]))\n",
    "udf_clear_hash = udf(lambda row: [x for x in row if '#' not in x], ArrayType(StringType(), True))\n",
    "udf_clear_user = udf(lambda row: [x for x in row if '@' not in x], ArrayType(StringType(), True))\n",
    "\n",
    "df_split = df_split\\\n",
    "    .withColumn('contain_tags', udf_cotains_hash_user('words'))\\\n",
    "    .withColumn('contain_hash_only', udf_contains_hash_only('words'))\\\n",
    "    .withColumn('words_clean', udf_clear_user(udf_clear_hash('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- contain_tags: string (nullable = true)\n",
      " |-- contain_hash_only: string (nullable = true)\n",
      " |-- words_clean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+-----------------+--------------------+\n",
      "|               words|label|contain_tags|contain_hash_only|         words_clean|\n",
      "+--------------------+-----+------------+-----------------+--------------------+\n",
      "|[#followfriday, @...|  pos|        true|             true|[for, being, top,...|\n",
      "|[@lambja, hey, ja...|  pos|        true|            false|[hey, james, how,...|\n",
      "|[@despiteofficial...|  pos|        true|            false|[we, had, a, list...|\n",
      "|[@sides, congrats, ]|  pos|        true|            false|        [congrats, ]|\n",
      "|[yeaaaah, yippppy...|  pos|       false|            false|[yeaaaah, yippppy...|\n",
      "|[@bhaktisbanter, ...|  pos|        true|             true|[this, one, is, i...|\n",
      "|[we, dont, like, ...|  pos|       false|            false|[we, dont, like, ...|\n",
      "|[@impatientraider...|  pos|        true|            false|[on, second, thou...|\n",
      "|[jgh, , but, we, ...|  pos|       false|            false|[jgh, , but, we, ...|\n",
      "|[as, an, act, of,...|  pos|       false|            false|[as, an, act, of,...|\n",
      "|[#followfriday, @...|  pos|        true|             true|[for, being, top,...|\n",
      "|[who, wouldnt, lo...|  pos|       false|            false|[who, wouldnt, lo...|\n",
      "|[@mish, , follow,...|  pos|        true|            false|[, follow, amp, h...|\n",
      "|[@jjulieredburn, ...|  pos|        true|            false|[perfect, so, you...|\n",
      "|[great, new, oppo...|  pos|       false|            false|[great, new, oppo...|\n",
      "|[laying, out, a, ...|  pos|       false|            false|[laying, out, a, ...|\n",
      "|[friends, lunch, ...|  pos|        true|             true|[friends, lunch, ...|\n",
      "|[@rookiesenpai, @...|  pos|        true|            false|[it, is, the, id,...|\n",
      "|[@oohdawg, hi, li...|  pos|        true|            false|         [hi, liv, ]|\n",
      "|[hello, i, need, ...|  pos|       false|            false|[hello, i, need, ...|\n",
      "+--------------------+-----+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding the Words into Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-----------------+\n",
      "|     word|label|contain_tags|contain_hash_only|\n",
      "+---------+-----+------------+-----------------+\n",
      "|      for|  pos|        true|             true|\n",
      "|    being|  pos|        true|             true|\n",
      "|      top|  pos|        true|             true|\n",
      "|  engaged|  pos|        true|             true|\n",
      "|  members|  pos|        true|             true|\n",
      "|       in|  pos|        true|             true|\n",
      "|       my|  pos|        true|             true|\n",
      "|community|  pos|        true|             true|\n",
      "|     this|  pos|        true|             true|\n",
      "|     week|  pos|        true|             true|\n",
      "|         |  pos|        true|             true|\n",
      "|      hey|  pos|        true|            false|\n",
      "|    james|  pos|        true|            false|\n",
      "|      how|  pos|        true|            false|\n",
      "|      odd|  pos|        true|            false|\n",
      "|         |  pos|        true|            false|\n",
      "|   please|  pos|        true|            false|\n",
      "|     call|  pos|        true|            false|\n",
      "|      our|  pos|        true|            false|\n",
      "|  contact|  pos|        true|            false|\n",
      "+---------+-----+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_split.select(explode('words_clean').alias('word'), 'label', 'contain_tags', 'contain_hash_only')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-----------------+\n",
      "|     word|label|contain_tags|contain_hash_only|\n",
      "+---------+-----+------------+-----------------+\n",
      "|      for|  pos|        true|             true|\n",
      "|    being|  pos|        true|             true|\n",
      "|      top|  pos|        true|             true|\n",
      "|  engaged|  pos|        true|             true|\n",
      "|  members|  pos|        true|             true|\n",
      "|       in|  pos|        true|             true|\n",
      "|       my|  pos|        true|             true|\n",
      "|community|  pos|        true|             true|\n",
      "|     this|  pos|        true|             true|\n",
      "|     week|  pos|        true|             true|\n",
      "|      hey|  pos|        true|            false|\n",
      "|    james|  pos|        true|            false|\n",
      "|      how|  pos|        true|            false|\n",
      "|      odd|  pos|        true|            false|\n",
      "|   please|  pos|        true|            false|\n",
      "|     call|  pos|        true|            false|\n",
      "|      our|  pos|        true|            false|\n",
      "|  contact|  pos|        true|            false|\n",
      "|   centre|  pos|        true|            false|\n",
      "|       on|  pos|        true|            false|\n",
      "+---------+-----+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.filter(col('word')!='')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|   community|  pos|\n",
      "|        week|  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         odd|  pos|\n",
      "|      please|  pos|\n",
      "|        call|  pos|\n",
      "|     contact|  pos|\n",
      "|      centre|  pos|\n",
      "|        able|  pos|\n",
      "|      assist|  pos|\n",
      "|        many|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "df_exp = df_exp.filter(~col('word').isin(stopset))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+\n",
      "|        word|label| id|\n",
      "+------------+-----+---+\n",
      "|followfriday|  pos|  0|\n",
      "|  franceinte|  pos|  1|\n",
      "|     pkuchly|  pos|  2|\n",
      "|milipolparis|  pos|  3|\n",
      "|         top|  pos|  4|\n",
      "|     engaged|  pos|  5|\n",
      "|     members|  pos|  6|\n",
      "|   community|  pos|  7|\n",
      "|        week|  pos|  8|\n",
      "|      lambja|  pos|  9|\n",
      "|         hey|  pos| 10|\n",
      "|       james|  pos| 11|\n",
      "|         odd|  pos| 12|\n",
      "|      please|  pos| 13|\n",
      "|        call|  pos| 14|\n",
      "|     contact|  pos| 15|\n",
      "|      centre|  pos| 16|\n",
      "|        able|  pos| 17|\n",
      "|      assist|  pos| 18|\n",
      "|        many|  pos| 19|\n",
      "+------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.withColumn('id', identity())\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.createOrReplaceTempView('WindowTutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    LAG(word, 1) OVER(ORDER BY id) AS w1,\n",
    "    word,\n",
    "    LEAD(word, 1) OVER(ORDER BY id) AS w2\n",
    "FROM WindowTutorial\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy('id')\n",
    "df_exp.select(\n",
    "    'id',\n",
    "    lag('word', 1).over(w).alias('w1'),\n",
    "    'word',\n",
    "    lead('word', 1).over(w).alias('w2')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window as Subquery: Most common 3-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|label|              w1|              w2|              w3|              w4|phrase_count|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|  pos|             amp|httptcorcvcyyoiq|          follow|               u|          62|\n",
      "|  pos|httptcorcvcyyoiq|          follow|               u|            back|          62|\n",
      "|  pos|          follow|         jnlazts|             amp|httptcorcvcyyoiq|          62|\n",
      "|  pos|         jnlazts|             amp|httptcorcvcyyoiq|          follow|          62|\n",
      "|  neg|        followed|          thanks| andjustinbieber|          please|          51|\n",
      "|  neg|          thanks| andjustinbieber|          please|        followed|          51|\n",
      "|  pos|              hi|             bam|   barsandmelody|          follow|          44|\n",
      "|  pos|           horan|           loves|             lot|             see|          44|\n",
      "|  pos|             see|          warsaw|              lt|            love|          44|\n",
      "|  pos|             bam|   barsandmelody|          follow|      bestfriend|          44|\n",
      "|  pos|   barsandmelody|          follow|      bestfriend|           horan|          44|\n",
      "|  pos|           loves|             lot|             see|          warsaw|          44|\n",
      "|  pos|           stats|             day|         arrived|             new|          44|\n",
      "|  pos|      bestfriend|           horan|           loves|             lot|          44|\n",
      "|  pos|             lot|             see|          warsaw|              lt|          44|\n",
      "|  pos|          follow|      bestfriend|           horan|           loves|          44|\n",
      "|  pos|          warsaw|              lt|            love|              lt|          44|\n",
      "|  pos|         arrived|             new|        follower|     unfollowers|          44|\n",
      "|  pos|             new|        follower|     unfollowers|             via|          44|\n",
      "|  pos|              lt|            love|              lt|               x|          43|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "GROUP BY label, w1, w2, w3, w4\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+--------+\n",
      "|          w1|           w2|                  w3|      w4|\n",
      "+------------+-------------+--------------------+--------+\n",
      "|zzzzzzplease|         dont|                 let|     sun|\n",
      "|        zzzz|       missed|                stop|    take|\n",
      "|   zzzterror|        didnt|                read|donation|\n",
      "|         zzz|      careful|            actually|   black|\n",
      "|         zzz|           xx|       physiotherapy|  friday|\n",
      "|          zz|      airport|            straight|    away|\n",
      "|     zysuzyy|       thanks|                  zy|    lets|\n",
      "|    zynovftw|  mrprowestie|         twoscotsmen|  simple|\n",
      "|          zy|         lets|              friend|     yaa|\n",
      "|     zxwlfxz|           hi|              adrian|  parcel|\n",
      "|    zupiapre|unfortunately|                 yes|    eveh|\n",
      "|       zumba|    somewhere|                else|  please|\n",
      "|   zulbayarb|         masa|nowadayshttptcoif...| brainer|\n",
      "|     zozeebo|       honest|                miss|   dubai|\n",
      "|     zozeebo|    tanyaburr|              please|    tell|\n",
      "|    zoyaashk|         true|            managers|     job|\n",
      "|    zoullgab|       frozen|               bagus|   emang|\n",
      "|   zouiriaii|         good|                luck|      ur|\n",
      "|zorroreturms|        cfvuf|                hope|  helped|\n",
      "|      zoradb|       texted|               dying|   lurgy|\n",
      "+------------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4\n",
    "FROM (\n",
    "    SELECT\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "ORDER BY w1 DESC, w2, w3, w4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|label|      w1|     w2|             w3|              w4|phrase_count|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|  neg|followed| thanks|andjustinbieber|          please|          51|\n",
      "|  pos|  follow|jnlazts|            amp|httptcorcvcyyoiq|          62|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH subquery_cte AS (\n",
    "    SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "    FROM (\n",
    "        SELECT\n",
    "            label,\n",
    "            word AS w1,\n",
    "            LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "            LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "            LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "        FROM WindowTutorial\n",
    "    )\n",
    "    GROUP BY label, w1, w2, w3, w4\n",
    ")\n",
    "SELECT label, w1, w2, w3, w4, phrase_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        ROW_NUMBER() OVER(PARTITION BY label ORDER BY phrase_count DESC) AS row,\n",
    "        w1, w2, w3, w4, phrase_count\n",
    "    FROM subquery_cte\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY label ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
