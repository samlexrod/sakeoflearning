{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import split, explode, regexp_replace, lower, regexp_extract\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id as identity, col, length\n",
    "from pyspark.sql.functions import lag, lead, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkNlp').getOrCreate()\n",
    "webbrowser.open('http://localhost:4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Twitter Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Twitter Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative documents\n",
    "pos_tw = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tw = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# joining documents\n",
    "document = [pos_tw] + [neg_tw]\n",
    "\n",
    "# list to dataframe\n",
    "df = pd.DataFrame(document[0]).append(pd.DataFrame(document[1])).rename(columns={0:'text', 1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Resilient Distributed Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df_rdd = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                          |label|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |pos  |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|pos  |\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |pos  |\n",
      "|@97sides CONGRATS :)                                                                                                          |pos  |\n",
      "|yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                    |pos  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creates a job\n",
    "df_rdd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp Table Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView('SqlNlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='sqlnlp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   sqlnlp|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |\n",
      "|@97sides CONGRATS :)                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.select('text').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Sentence Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('label').orderBy('text')\n",
    "df_rdd = df_rdd.withColumn('sentence_id', row_number().over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Invalid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                      |label|sentence_id|clean_text                                                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|!! Quick notice regarding requests. Our DM is now open for people to request moments/ideas for tweets, thank you :) http://t.co/joEpeCsq29|pos  |1          | quick notice regarding requests our dm is now open for people to request momentsideas for tweets thank you  httptcojoepecsq|\n",
      "|\"@CassTheTrainer: A Huge &amp; Warm Welcome to @VodkaBlond :-))\"\n",
      "\n",
      "☆ finally we complete the triangle ! ☆                                  |pos  |2          |@cassthetrainer a huge amp warm welcome to @vodkablond  finally we complete the triangle                                    |\n",
      "|\"@CowokAddict: Mama is the only reason why I stand stronger up to now! :)\"                                                                |pos  |3          |@cowokaddict mama is the only reason why i stand stronger up to now                                                         |\n",
      "Yeah ! Sometimes.                                                                 |pos  |4          |@katiiirocks @miangeorges are u a beautiful man dyeah  sometimes                                                            |\n",
      "|\"@Manuellatchgn: Goodbye twitter. I will not be there for a long time. :)\" Goodbye Manuella                                               |pos  |5          |@manuellatchgn goodbye twitter i will not be there for a long time  goodbye manuella                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_rdd.withColumn('clean_text', lower(regexp_replace('text', '[^a-zA-Z#@ ]', '')))\n",
    "df_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "|words                                                                                                                                             |label|sentence_id|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "|[, quick, notice, regarding, requests, our, dm, is, now, open, for, people, to, request, momentsideas, for, tweets, thank, you, , httptcojoepecsq]|pos  |1          |\n",
      "|[@cassthetrainer, a, huge, amp, warm, welcome, to, @vodkablond, , finally, we, complete, the, triangle, , ]                                       |pos  |2          |\n",
      "|[@cowokaddict, mama, is, the, only, reason, why, i, stand, stronger, up, to, now, ]                                                               |pos  |3          |\n",
      "|[@katiiirocks, @miangeorges, are, u, a, beautiful, man, dyeah, , sometimes]                                                                       |pos  |4          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split = df_clean.select(split('clean_text', ' ').alias('words'), 'label', 'sentence_id')\n",
    "df_split.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Hash and User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cotains_hash_user = udf(lambda row: any([any([i in x for i in ['#', '@']]) for x in row]))\n",
    "udf_contains_hash_only = udf(lambda row: any(['#' in x for x in row]))\n",
    "udf_clear_hash = udf(lambda row: [x for x in row if '#' not in x], ArrayType(StringType(), True))\n",
    "udf_clear_user = udf(lambda row: [x for x in row if '@' not in x], ArrayType(StringType(), True))\n",
    "\n",
    "df_split = df_split\\\n",
    "    .withColumn('contain_tags', udf_cotains_hash_user('words'))\\\n",
    "    .withColumn('contain_hash_only', udf_contains_hash_only('words'))\\\n",
    "    .withColumn('words_clean', udf_clear_user(udf_clear_hash('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- sentence_id: integer (nullable = true)\n",
      " |-- contain_tags: string (nullable = true)\n",
      " |-- contain_hash_only: string (nullable = true)\n",
      " |-- words_clean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "|               words|label|sentence_id|contain_tags|contain_hash_only|         words_clean|\n",
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "|[, quick, notice,...|  pos|          1|       false|            false|[, quick, notice,...|\n",
      "|[@cassthetrainer,...|  pos|          2|        true|            false|[a, huge, amp, wa...|\n",
      "|[@cowokaddict, ma...|  pos|          3|        true|            false|[mama, is, the, o...|\n",
      "|[@katiiirocks, @m...|  pos|          4|        true|            false|[are, u, a, beaut...|\n",
      "|[@manuellatchgn, ...|  pos|          5|        true|            false|[goodbye, twitter...|\n",
      "|[@nyesekkinn, don...|  pos|          6|        true|            false|[dont, be, affara...|\n",
      "|[@realliampayne, ...|  pos|          7|        true|            false|[yeah, thanks, fo...|\n",
      "|[@southafrica, @c...|  pos|          8|        true|             true|[visit, for, cult...|\n",
      "|[@teambailonaofc,...|  pos|          9|        true|            false|[rt, to, be, in, ...|\n",
      "|[@divarh, @graceg...|  pos|         10|        true|            false|[seems, like, you...|\n",
      "|[@fireddestiny, #...|  pos|         11|        true|             true|[im, a, huge, fan...|\n",
      "|[@fireddestiny, #...|  pos|         12|        true|             true|[im, a, huge, fan...|\n",
      "|[@jaytjubby, @sin...|  pos|         13|        true|            false|[yes, hun, and, y...|\n",
      "|[@justinbieber, w...|  pos|         14|        true|            false|         [why, baby]|\n",
      "|[@macykatemusic, ...|  pos|         15|        true|            false|[you, know, you, ...|\n",
      "|[@yettygeers, god...|  pos|         16|        true|            false|[god, has, a, pla...|\n",
      "|[@yettygeers, we,...|  pos|         17|        true|            false|[we, are, saved, ...|\n",
      "|[@yettygeers, whe...|  pos|         18|        true|            false|[when, god, gives...|\n",
      "|[@yettygeers, don...|  pos|         19|        true|            false|[dont, give, up, ...|\n",
      "|[@yettygeers, in,...|  pos|         20|        true|            false|[in, good, and, i...|\n",
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding the Words into Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-----------------+\n",
      "|     word|label|contain_tags|contain_hash_only|\n",
      "+---------+-----+------------+-----------------+\n",
      "|      for|  pos|        true|             true|\n",
      "|    being|  pos|        true|             true|\n",
      "|      top|  pos|        true|             true|\n",
      "|  engaged|  pos|        true|             true|\n",
      "|  members|  pos|        true|             true|\n",
      "|       in|  pos|        true|             true|\n",
      "|       my|  pos|        true|             true|\n",
      "|community|  pos|        true|             true|\n",
      "|     this|  pos|        true|             true|\n",
      "|     week|  pos|        true|             true|\n",
      "|         |  pos|        true|             true|\n",
      "|      hey|  pos|        true|            false|\n",
      "|    james|  pos|        true|            false|\n",
      "|      how|  pos|        true|            false|\n",
      "|      odd|  pos|        true|            false|\n",
      "|         |  pos|        true|            false|\n",
      "|   please|  pos|        true|            false|\n",
      "|     call|  pos|        true|            false|\n",
      "|      our|  pos|        true|            false|\n",
      "|  contact|  pos|        true|            false|\n",
      "+---------+-----+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_split.select(explode('words_clean').alias('word'), 'label', 'contain_tags', 'contain_hash_only')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-----------------+\n",
      "|     word|label|contain_tags|contain_hash_only|\n",
      "+---------+-----+------------+-----------------+\n",
      "|      for|  pos|        true|             true|\n",
      "|    being|  pos|        true|             true|\n",
      "|      top|  pos|        true|             true|\n",
      "|  engaged|  pos|        true|             true|\n",
      "|  members|  pos|        true|             true|\n",
      "|       in|  pos|        true|             true|\n",
      "|       my|  pos|        true|             true|\n",
      "|community|  pos|        true|             true|\n",
      "|     this|  pos|        true|             true|\n",
      "|     week|  pos|        true|             true|\n",
      "|      hey|  pos|        true|            false|\n",
      "|    james|  pos|        true|            false|\n",
      "|      how|  pos|        true|            false|\n",
      "|      odd|  pos|        true|            false|\n",
      "|   please|  pos|        true|            false|\n",
      "|     call|  pos|        true|            false|\n",
      "|      our|  pos|        true|            false|\n",
      "|  contact|  pos|        true|            false|\n",
      "|   centre|  pos|        true|            false|\n",
      "|       on|  pos|        true|            false|\n",
      "+---------+-----+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.filter(col('word')!='')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+\n",
      "|        word|label| id|\n",
      "+------------+-----+---+\n",
      "|followfriday|  pos|  0|\n",
      "|  franceinte|  pos|  1|\n",
      "|     pkuchly|  pos|  2|\n",
      "|milipolparis|  pos|  3|\n",
      "|         top|  pos|  4|\n",
      "|     engaged|  pos|  5|\n",
      "|     members|  pos|  6|\n",
      "|   community|  pos|  7|\n",
      "|        week|  pos|  8|\n",
      "|      lambja|  pos|  9|\n",
      "|         hey|  pos| 10|\n",
      "|       james|  pos| 11|\n",
      "|         odd|  pos| 12|\n",
      "|      please|  pos| 13|\n",
      "|        call|  pos| 14|\n",
      "|     contact|  pos| 15|\n",
      "|      centre|  pos| 16|\n",
      "|        able|  pos| 17|\n",
      "|      assist|  pos| 18|\n",
      "|        many|  pos| 19|\n",
      "+------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.withColumn('id', identity())\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.createOrReplaceTempView('WindowTutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    LAG(word, 1) OVER(ORDER BY id) AS w1,\n",
    "    word,\n",
    "    LEAD(word, 1) OVER(ORDER BY id) AS w2\n",
    "FROM WindowTutorial\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+------------+\n",
      "| id|          w1|        word|          w2|\n",
      "+---+------------+------------+------------+\n",
      "|  0|        null|followfriday|  franceinte|\n",
      "|  1|followfriday|  franceinte|     pkuchly|\n",
      "|  2|  franceinte|     pkuchly|milipolparis|\n",
      "|  3|     pkuchly|milipolparis|         top|\n",
      "|  4|milipolparis|         top|     engaged|\n",
      "|  5|         top|     engaged|     members|\n",
      "|  6|     engaged|     members|   community|\n",
      "|  7|     members|   community|        week|\n",
      "|  8|   community|        week|      lambja|\n",
      "|  9|        week|      lambja|         hey|\n",
      "| 10|      lambja|         hey|       james|\n",
      "| 11|         hey|       james|         odd|\n",
      "| 12|       james|         odd|      please|\n",
      "| 13|         odd|      please|        call|\n",
      "| 14|      please|        call|     contact|\n",
      "| 15|        call|     contact|      centre|\n",
      "| 16|     contact|      centre|        able|\n",
      "| 17|      centre|        able|      assist|\n",
      "| 18|        able|      assist|        many|\n",
      "| 19|      assist|        many|      thanks|\n",
      "+---+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy('id')\n",
    "df_exp.select(\n",
    "    'id',\n",
    "    lag('word', 1).over(w).alias('w1'),\n",
    "    'word',\n",
    "    lead('word', 1).over(w).alias('w2')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window as Subquery: Most common 3-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|label|              w1|              w2|              w3|              w4|phrase_count|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "|  pos|             amp|httptcorcvcyyoiq|          follow|               u|          62|\n",
      "|  pos|httptcorcvcyyoiq|          follow|               u|            back|          62|\n",
      "|  pos|          follow|         jnlazts|             amp|httptcorcvcyyoiq|          62|\n",
      "|  pos|         jnlazts|             amp|httptcorcvcyyoiq|          follow|          62|\n",
      "|  neg|        followed|          thanks| andjustinbieber|          please|          51|\n",
      "|  neg|          thanks| andjustinbieber|          please|        followed|          51|\n",
      "|  pos|              hi|             bam|   barsandmelody|          follow|          44|\n",
      "|  pos|           horan|           loves|             lot|             see|          44|\n",
      "|  pos|             see|          warsaw|              lt|            love|          44|\n",
      "|  pos|             bam|   barsandmelody|          follow|      bestfriend|          44|\n",
      "|  pos|   barsandmelody|          follow|      bestfriend|           horan|          44|\n",
      "|  pos|           loves|             lot|             see|          warsaw|          44|\n",
      "|  pos|           stats|             day|         arrived|             new|          44|\n",
      "|  pos|      bestfriend|           horan|           loves|             lot|          44|\n",
      "|  pos|             lot|             see|          warsaw|              lt|          44|\n",
      "|  pos|          follow|      bestfriend|           horan|           loves|          44|\n",
      "|  pos|          warsaw|              lt|            love|              lt|          44|\n",
      "|  pos|         arrived|             new|        follower|     unfollowers|          44|\n",
      "|  pos|             new|        follower|     unfollowers|             via|          44|\n",
      "|  pos|              lt|            love|              lt|               x|          43|\n",
      "+-----+----------------+----------------+----------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "GROUP BY label, w1, w2, w3, w4\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------------+--------+\n",
      "|          w1|           w2|                  w3|      w4|\n",
      "+------------+-------------+--------------------+--------+\n",
      "|zzzzzzplease|         dont|                 let|     sun|\n",
      "|        zzzz|       missed|                stop|    take|\n",
      "|   zzzterror|        didnt|                read|donation|\n",
      "|         zzz|      careful|            actually|   black|\n",
      "|         zzz|           xx|       physiotherapy|  friday|\n",
      "|          zz|      airport|            straight|    away|\n",
      "|     zysuzyy|       thanks|                  zy|    lets|\n",
      "|    zynovftw|  mrprowestie|         twoscotsmen|  simple|\n",
      "|          zy|         lets|              friend|     yaa|\n",
      "|     zxwlfxz|           hi|              adrian|  parcel|\n",
      "|    zupiapre|unfortunately|                 yes|    eveh|\n",
      "|       zumba|    somewhere|                else|  please|\n",
      "|   zulbayarb|         masa|nowadayshttptcoif...| brainer|\n",
      "|     zozeebo|       honest|                miss|   dubai|\n",
      "|     zozeebo|    tanyaburr|              please|    tell|\n",
      "|    zoyaashk|         true|            managers|     job|\n",
      "|    zoullgab|       frozen|               bagus|   emang|\n",
      "|   zouiriaii|         good|                luck|      ur|\n",
      "|zorroreturms|        cfvuf|                hope|  helped|\n",
      "|      zoradb|       texted|               dying|   lurgy|\n",
      "+------------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4\n",
    "FROM (\n",
    "    SELECT\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "ORDER BY w1 DESC, w2, w3, w4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|label|      w1|     w2|             w3|              w4|phrase_count|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "|  neg|followed| thanks|andjustinbieber|          please|          51|\n",
      "|  pos|  follow|jnlazts|            amp|httptcorcvcyyoiq|          62|\n",
      "+-----+--------+-------+---------------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH subquery_cte AS (\n",
    "    SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "    FROM (\n",
    "        SELECT\n",
    "            label,\n",
    "            word AS w1,\n",
    "            LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "            LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "            LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "        FROM WindowTutorial\n",
    "    )\n",
    "    GROUP BY label, w1, w2, w3, w4\n",
    ")\n",
    "SELECT label, w1, w2, w3, w4, phrase_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        ROW_NUMBER() OVER(PARTITION BY label ORDER BY phrase_count DESC) AS row,\n",
    "        w1, w2, w3, w4, phrase_count\n",
    "    FROM subquery_cte\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY label ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|   community|  pos|\n",
      "|        week|  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         odd|  pos|\n",
      "|      please|  pos|\n",
      "|        call|  pos|\n",
      "|     contact|  pos|\n",
      "|      centre|  pos|\n",
      "|        able|  pos|\n",
      "|      assist|  pos|\n",
      "|        many|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "df_exp = df_exp.filter(~col('word').isin(stopset))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
