{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import split, explode, regexp_replace, lower, regexp_extract\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id as identity, col, length\n",
    "from pyspark.sql.functions import lag, lead, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkNlp').getOrCreate()\n",
    "webbrowser.open('http://localhost:4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Twitter Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Twitter Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative documents\n",
    "pos_tw = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tw = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# joining documents\n",
    "document = [pos_tw] + [neg_tw]\n",
    "\n",
    "# list to dataframe\n",
    "df = pd.DataFrame(document[0]).append(pd.DataFrame(document[1])).rename(columns={0:'text', 1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Resilient Distributed Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df_rdd = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                          |label|\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |pos  |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|pos  |\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |pos  |\n",
      "|@97sides CONGRATS :)                                                                                                          |pos  |\n",
      "|yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days                    |pos  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creates a job\n",
    "df_rdd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp Table Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView('SqlNlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='sqlnlp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |   sqlnlp|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)               |\n",
      "|@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!|\n",
      "|@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!                   |\n",
      "|@97sides CONGRATS :)                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rdd.select('text').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Sentence Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('label').orderBy('text')\n",
    "df_rdd = df_rdd.withColumn('sentence_id', row_number().over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Invalid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                      |label|sentence_id|clean_text                                                                                                                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|!! Quick notice regarding requests. Our DM is now open for people to request moments/ideas for tweets, thank you :) http://t.co/joEpeCsq29|pos  |1          | quick notice regarding requests our dm is now open for people to request momentsideas for tweets thank you  httptcojoepecsq|\n",
      "|\"@CassTheTrainer: A Huge &amp; Warm Welcome to @VodkaBlond :-))\"\n",
      "\n",
      "☆ finally we complete the triangle ! ☆                                  |pos  |2          |@cassthetrainer a huge amp warm welcome to @vodkablond  finally we complete the triangle                                    |\n",
      "|\"@CowokAddict: Mama is the only reason why I stand stronger up to now! :)\"                                                                |pos  |3          |@cowokaddict mama is the only reason why i stand stronger up to now                                                         |\n",
      "Yeah ! Sometimes.                                                                 |pos  |4          |@katiiirocks @miangeorges are u a beautiful man dyeah  sometimes                                                            |\n",
      "|\"@Manuellatchgn: Goodbye twitter. I will not be there for a long time. :)\" Goodbye Manuella                                               |pos  |5          |@manuellatchgn goodbye twitter i will not be there for a long time  goodbye manuella                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean = df_rdd.withColumn('clean_text', lower(regexp_replace('text', '[^a-zA-Z#@ ]', '')))\n",
    "df_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "|words                                                                                                                                             |label|sentence_id|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "|[, quick, notice, regarding, requests, our, dm, is, now, open, for, people, to, request, momentsideas, for, tweets, thank, you, , httptcojoepecsq]|pos  |1          |\n",
      "|[@cassthetrainer, a, huge, amp, warm, welcome, to, @vodkablond, , finally, we, complete, the, triangle, , ]                                       |pos  |2          |\n",
      "|[@cowokaddict, mama, is, the, only, reason, why, i, stand, stronger, up, to, now, ]                                                               |pos  |3          |\n",
      "|[@katiiirocks, @miangeorges, are, u, a, beautiful, man, dyeah, , sometimes]                                                                       |pos  |4          |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------+-----+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split = df_clean.select(split('clean_text', ' ').alias('words'), 'label', 'sentence_id')\n",
    "df_split.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Hash and User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cotains_hash_user = udf(lambda row: any([any([i in x for i in ['#', '@']]) for x in row]))\n",
    "udf_contains_hash_only = udf(lambda row: any(['#' in x for x in row]))\n",
    "udf_clear_hash = udf(lambda row: [x for x in row if '#' not in x], ArrayType(StringType(), True))\n",
    "udf_clear_user = udf(lambda row: [x for x in row if '@' not in x], ArrayType(StringType(), True))\n",
    "\n",
    "df_split = df_split\\\n",
    "    .withColumn('contain_tags', udf_cotains_hash_user('words'))\\\n",
    "    .withColumn('contain_hash_only', udf_contains_hash_only('words'))\\\n",
    "    .withColumn('words_clean', udf_clear_user(udf_clear_hash('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- sentence_id: integer (nullable = true)\n",
      " |-- contain_tags: string (nullable = true)\n",
      " |-- contain_hash_only: string (nullable = true)\n",
      " |-- words_clean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "|               words|label|sentence_id|contain_tags|contain_hash_only|         words_clean|\n",
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "|[, quick, notice,...|  pos|          1|       false|            false|[, quick, notice,...|\n",
      "|[@cassthetrainer,...|  pos|          2|        true|            false|[a, huge, amp, wa...|\n",
      "|[@cowokaddict, ma...|  pos|          3|        true|            false|[mama, is, the, o...|\n",
      "|[@katiiirocks, @m...|  pos|          4|        true|            false|[are, u, a, beaut...|\n",
      "|[@manuellatchgn, ...|  pos|          5|        true|            false|[goodbye, twitter...|\n",
      "|[@nyesekkinn, don...|  pos|          6|        true|            false|[dont, be, affara...|\n",
      "|[@realliampayne, ...|  pos|          7|        true|            false|[yeah, thanks, fo...|\n",
      "|[@southafrica, @c...|  pos|          8|        true|             true|[visit, for, cult...|\n",
      "|[@teambailonaofc,...|  pos|          9|        true|            false|[rt, to, be, in, ...|\n",
      "|[@divarh, @graceg...|  pos|         10|        true|            false|[seems, like, you...|\n",
      "|[@fireddestiny, #...|  pos|         11|        true|             true|[im, a, huge, fan...|\n",
      "|[@fireddestiny, #...|  pos|         12|        true|             true|[im, a, huge, fan...|\n",
      "|[@jaytjubby, @sin...|  pos|         13|        true|            false|[yes, hun, and, y...|\n",
      "|[@justinbieber, w...|  pos|         14|        true|            false|         [why, baby]|\n",
      "|[@macykatemusic, ...|  pos|         15|        true|            false|[you, know, you, ...|\n",
      "|[@yettygeers, god...|  pos|         16|        true|            false|[god, has, a, pla...|\n",
      "|[@yettygeers, we,...|  pos|         17|        true|            false|[we, are, saved, ...|\n",
      "|[@yettygeers, whe...|  pos|         18|        true|            false|[when, god, gives...|\n",
      "|[@yettygeers, don...|  pos|         19|        true|            false|[dont, give, up, ...|\n",
      "|[@yettygeers, in,...|  pos|         20|        true|            false|[in, good, and, i...|\n",
      "+--------------------+-----+-----------+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding the Words into Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------------+-----------------+-----------+\n",
      "|        word|label|contain_tags|contain_hash_only|sentence_id|\n",
      "+------------+-----+------------+-----------------+-----------+\n",
      "|            |  pos|       false|            false|          1|\n",
      "|       quick|  pos|       false|            false|          1|\n",
      "|      notice|  pos|       false|            false|          1|\n",
      "|   regarding|  pos|       false|            false|          1|\n",
      "|    requests|  pos|       false|            false|          1|\n",
      "|         our|  pos|       false|            false|          1|\n",
      "|          dm|  pos|       false|            false|          1|\n",
      "|          is|  pos|       false|            false|          1|\n",
      "|         now|  pos|       false|            false|          1|\n",
      "|        open|  pos|       false|            false|          1|\n",
      "|         for|  pos|       false|            false|          1|\n",
      "|      people|  pos|       false|            false|          1|\n",
      "|          to|  pos|       false|            false|          1|\n",
      "|     request|  pos|       false|            false|          1|\n",
      "|momentsideas|  pos|       false|            false|          1|\n",
      "|         for|  pos|       false|            false|          1|\n",
      "|      tweets|  pos|       false|            false|          1|\n",
      "|       thank|  pos|       false|            false|          1|\n",
      "|         you|  pos|       false|            false|          1|\n",
      "|            |  pos|       false|            false|          1|\n",
      "+------------+-----+------------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_split.select(explode('words_clean').alias('word'), 'label', 'contain_tags', 'contain_hash_only', 'sentence_id')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache in memory to avoid lazy evaluation later\n",
    "df_exp.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------------+-----------------+\n",
      "|     word|label|contain_tags|contain_hash_only|\n",
      "+---------+-----+------------+-----------------+\n",
      "|      for|  pos|        true|             true|\n",
      "|    being|  pos|        true|             true|\n",
      "|      top|  pos|        true|             true|\n",
      "|  engaged|  pos|        true|             true|\n",
      "|  members|  pos|        true|             true|\n",
      "|       in|  pos|        true|             true|\n",
      "|       my|  pos|        true|             true|\n",
      "|community|  pos|        true|             true|\n",
      "|     this|  pos|        true|             true|\n",
      "|     week|  pos|        true|             true|\n",
      "|      hey|  pos|        true|            false|\n",
      "|    james|  pos|        true|            false|\n",
      "|      how|  pos|        true|            false|\n",
      "|      odd|  pos|        true|            false|\n",
      "|   please|  pos|        true|            false|\n",
      "|     call|  pos|        true|            false|\n",
      "|      our|  pos|        true|            false|\n",
      "|  contact|  pos|        true|            false|\n",
      "|   centre|  pos|        true|            false|\n",
      "|       on|  pos|        true|            false|\n",
      "+---------+-----+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.filter(col('word')!='')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------------+-----------------+------------+\n",
      "|           word|label|contain_tags|contain_hash_only|          id|\n",
      "+---------------+-----+------------+-----------------+------------+\n",
      "|          quick|  pos|       false|            false|300647710720|\n",
      "|         notice|  pos|       false|            false|300647710721|\n",
      "|      regarding|  pos|       false|            false|300647710722|\n",
      "|       requests|  pos|       false|            false|300647710723|\n",
      "|            our|  pos|       false|            false|300647710724|\n",
      "|             dm|  pos|       false|            false|300647710725|\n",
      "|             is|  pos|       false|            false|300647710726|\n",
      "|            now|  pos|       false|            false|300647710727|\n",
      "|           open|  pos|       false|            false|300647710728|\n",
      "|            for|  pos|       false|            false|300647710729|\n",
      "|         people|  pos|       false|            false|300647710730|\n",
      "|             to|  pos|       false|            false|300647710731|\n",
      "|        request|  pos|       false|            false|300647710732|\n",
      "|   momentsideas|  pos|       false|            false|300647710733|\n",
      "|            for|  pos|       false|            false|300647710734|\n",
      "|         tweets|  pos|       false|            false|300647710735|\n",
      "|          thank|  pos|       false|            false|300647710736|\n",
      "|            you|  pos|       false|            false|300647710737|\n",
      "|httptcojoepecsq|  pos|       false|            false|300647710738|\n",
      "|              a|  pos|        true|            false|300647710739|\n",
      "+---------------+-----+------------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp = df_exp.withColumn('id', identity())\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.createOrReplaceTempView('WindowTutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+\n",
      "| id|       w1|     word|       w2|\n",
      "+---+---------+---------+---------+\n",
      "|  0|     null|      for|    being|\n",
      "|  1|      for|    being|      top|\n",
      "|  2|    being|      top|  engaged|\n",
      "|  3|      top|  engaged|  members|\n",
      "|  4|  engaged|  members|       in|\n",
      "|  5|  members|       in|       my|\n",
      "|  6|       in|       my|community|\n",
      "|  7|       my|community|     this|\n",
      "|  8|community|     this|     week|\n",
      "|  9|     this|     week|      hey|\n",
      "| 10|     week|      hey|    james|\n",
      "| 11|      hey|    james|      how|\n",
      "| 12|    james|      how|      odd|\n",
      "| 13|      how|      odd|   please|\n",
      "| 14|      odd|   please|     call|\n",
      "| 15|   please|     call|      our|\n",
      "| 16|     call|      our|  contact|\n",
      "| 17|      our|  contact|   centre|\n",
      "| 18|  contact|   centre|       on|\n",
      "| 19|   centre|       on|      and|\n",
      "+---+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    LAG(word, 1) OVER(ORDER BY id) AS w1,\n",
    "    word,\n",
    "    LEAD(word, 1) OVER(ORDER BY id) AS w2\n",
    "FROM WindowTutorial\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+\n",
      "| id|       w1|     word|       w2|\n",
      "+---+---------+---------+---------+\n",
      "|  0|     null|      for|    being|\n",
      "|  1|      for|    being|      top|\n",
      "|  2|    being|      top|  engaged|\n",
      "|  3|      top|  engaged|  members|\n",
      "|  4|  engaged|  members|       in|\n",
      "|  5|  members|       in|       my|\n",
      "|  6|       in|       my|community|\n",
      "|  7|       my|community|     this|\n",
      "|  8|community|     this|     week|\n",
      "|  9|     this|     week|      hey|\n",
      "| 10|     week|      hey|    james|\n",
      "| 11|      hey|    james|      how|\n",
      "| 12|    james|      how|      odd|\n",
      "| 13|      how|      odd|   please|\n",
      "| 14|      odd|   please|     call|\n",
      "| 15|   please|     call|      our|\n",
      "| 16|     call|      our|  contact|\n",
      "| 17|      our|  contact|   centre|\n",
      "| 18|  contact|   centre|       on|\n",
      "| 19|   centre|       on|      and|\n",
      "+---+---------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w = Window.orderBy('id')\n",
    "df_exp.select(\n",
    "    'id',\n",
    "    lag('word', 1).over(w).alias('w1'),\n",
    "    'word',\n",
    "    lead('word', 1).over(w).alias('w2')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window as Subquery: Most common 3-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+----------------+----------------+----------+------------+\n",
      "|label|              w1|              w2|              w3|        w4|phrase_count|\n",
      "+-----+----------------+----------------+----------------+----------+------------+\n",
      "|  pos|httptcorcvcyyoiq|          follow|               u|      back|          62|\n",
      "|  pos|             amp|httptcorcvcyyoiq|          follow|         u|          62|\n",
      "|  pos|          follow|             amp|httptcorcvcyyoiq|    follow|          62|\n",
      "|  pos|             and|              no|     unfollowers|       via|          60|\n",
      "|  neg|        followed|              me|          thanks|    please|          51|\n",
      "|  neg|              me|          thanks|          please|  followed|          51|\n",
      "|  neg|          thanks|          please|        followed|        me|          51|\n",
      "|  neg|          please|        followed|              me|       too|          51|\n",
      "|  pos|             she|           loves|             you|         a|          44|\n",
      "|  pos|           loves|             you|               a|       lot|          44|\n",
      "|  pos|             you|               a|             lot|       see|          44|\n",
      "|  pos|             lot|             see|             you|        in|          44|\n",
      "|  pos|      bestfriend|             she|           loves|       you|          44|\n",
      "|  pos|           stats|             for|             the|       day|          44|\n",
      "|  pos|               a|             lot|             see|       you|          44|\n",
      "|  pos|             you|          follow|              my|bestfriend|          44|\n",
      "|  pos|             see|             you|              in|    warsaw|          44|\n",
      "|  pos|             you|              in|          warsaw|        lt|          44|\n",
      "|  pos|             bam|             can|             you|    follow|          44|\n",
      "|  pos|              my|      bestfriend|             she|     loves|          44|\n",
      "+-----+----------------+----------------+----------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "GROUP BY label, w1, w2, w3, w4\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+-----------+\n",
      "|          w1|       w2|           w3|         w4|\n",
      "+------------+---------+-------------+-----------+\n",
      "|zzzzzzplease|     dont|          let|        the|\n",
      "|        zzzz|   missed|           my|       stop|\n",
      "|         zzz|      how|      careful|         do|\n",
      "|         zzz|       xx|physiotherapy|     friday|\n",
      "|          zz|     from|      airport|   straight|\n",
      "|          zy|     lets|           be|     friend|\n",
      "|       zumba|somewhere|         else|     please|\n",
      "|   zopiclone|      for|        sleep|         in|\n",
      "|      zoomed|     away|       damnit|          i|\n",
      "|        zoom|     into|         hers|       lmao|\n",
      "|         zoo|      and|          its|       only|\n",
      "|         zoo|     baby|           gt|  boyfriend|\n",
      "|    zonzofox|      app|        using|       this|\n",
      "|    zonzofox| clicking|         this|       link|\n",
      "|        zone|      and|            i|      could|\n",
      "|        zone|       my|     snapchat|leanneriner|\n",
      "|       zokay|  russian|       accent|         so|\n",
      "|         zoe|      are|         well|       mate|\n",
      "|         zoe|     katy|        jaycy|        jen|\n",
      "|      zodiac|     sign|           is|       same|\n",
      "+------------+---------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4\n",
    "FROM (\n",
    "    SELECT\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "ORDER BY w1 DESC, w2, w3, w4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+----------------+------+------------+\n",
      "|label|      w1| w2|              w3|    w4|phrase_count|\n",
      "+-----+--------+---+----------------+------+------------+\n",
      "|  neg|followed| me|          thanks|please|          51|\n",
      "|  pos|  follow|amp|httptcorcvcyyoiq|follow|          62|\n",
      "+-----+--------+---+----------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH subquery_cte AS (\n",
    "    SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "    FROM (\n",
    "        SELECT\n",
    "            label,\n",
    "            word AS w1,\n",
    "            LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "            LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "            LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "        FROM WindowTutorial\n",
    "    )\n",
    "    GROUP BY label, w1, w2, w3, w4\n",
    ")\n",
    "SELECT label, w1, w2, w3, w4, phrase_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        ROW_NUMBER() OVER(PARTITION BY label ORDER BY phrase_count DESC) AS row,\n",
    "        w1, w2, w3, w4, phrase_count\n",
    "    FROM subquery_cte\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY label ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, label: string, contain_tags: string, contain_hash_only: string, id: bigint]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_exp.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------+-----------------+----+\n",
      "|            word|label|contain_tags|contain_hash_only|  id|\n",
      "+----------------+-----+------------+-----------------+----+\n",
      "|httptcorcvcyyoiq|  pos|        true|            false| 161|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false| 393|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false| 649|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false| 927|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|1177|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|1391|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|1549|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|1764|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|2017|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|2406|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|2618|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|2837|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|2983|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|3243|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|3444|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|3689|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|3871|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|4036|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|4175|\n",
      "|httptcorcvcyyoiq|  pos|        true|            false|4680|\n",
      "+----------------+-----+------------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_exp.filter(col('word')=='httptcorcvcyyoiq').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sammy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|label|\n",
      "+------------+-----+\n",
      "|followfriday|  pos|\n",
      "|  franceinte|  pos|\n",
      "|     pkuchly|  pos|\n",
      "|milipolparis|  pos|\n",
      "|         top|  pos|\n",
      "|     engaged|  pos|\n",
      "|     members|  pos|\n",
      "|   community|  pos|\n",
      "|        week|  pos|\n",
      "|      lambja|  pos|\n",
      "|         hey|  pos|\n",
      "|       james|  pos|\n",
      "|         odd|  pos|\n",
      "|      please|  pos|\n",
      "|        call|  pos|\n",
      "|     contact|  pos|\n",
      "|      centre|  pos|\n",
      "|        able|  pos|\n",
      "|      assist|  pos|\n",
      "|        many|  pos|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "df_exp = df_exp.filter(~col('word').isin(stopset))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
