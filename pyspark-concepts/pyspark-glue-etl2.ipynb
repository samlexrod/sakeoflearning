{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from s3fs import S3FileSystem\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'BCMA'\n",
    "received_date = '2019-01-01'\n",
    "\n",
    "# declaring variables\n",
    "client_id = client_id.lower()\n",
    "received_date = 'RD-'+pd.to_datetime(received_date).strftime(\"%Y-%m-%d\")\n",
    "database = client_id+'_'+received_date\n",
    "s3_path = os.path.join('s3://prospect-raw-files/', client_id, received_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write Functions from CSV to Parquet    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_objects():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    all_objects = s3.list_objects(Bucket='prospect-raw-files')\n",
    "    \n",
    "    target_files = lambda key: '.csv' in key and received_date in key\n",
    "    target_keys = list(filter(target_files, [cont.get('Key') for cont in all_objects['Contents']]))\n",
    "    \n",
    "    obj_dict = {'Body': [], 'Key': [], 'DelKey': []}\n",
    "    for key in target_keys:\n",
    "        obj = s3.get_object(Bucket='prospect-raw-files', Key=key)\n",
    "        obj_dict['Body'].append(obj['Body'])\n",
    "        obj_dict['Key'].append(os.path.dirname(key))\n",
    "        obj_dict['DelKey'].append(key)\n",
    "    return obj_dict\n",
    "\n",
    "def write_parquet(obj_dict, delete_source=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    objs_keys = list(zip(target_obj.get('Body'), target_obj.get('Key'), target_obj.get('DelKey')))\n",
    "    for obj, key, delkey in objs_keys:\n",
    "        print(obj)\n",
    "        df = pd.read_csv(obj)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        print(os.path.join('s3://', key))\n",
    "        pq.write_to_dataset(\n",
    "            table=table, \n",
    "            root_path=os.path.join('s3://prospect-raw-files', key), \n",
    "            filesystem=S3FileSystem()\n",
    "        ) \n",
    "        print(delkey)\n",
    "\n",
    "        if delete_source:\n",
    "            s3.delete_object(Bucket='prospect-raw-files', Key=delkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s3: Start with File Convertion to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert all csv files to parquet files  (delete or move raw files to cold storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_obj = read_objects()\n",
    "write_parquet(target_obj, delete_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glue: Continue with Ad-Hoc Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Ad-Hoc Database\n",
    "This database is designed to catalogue raw tables related to this client. For this process, if only one table is created, then the data is ready to be transformed to parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': database\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the Ad-Hoc Crawler\n",
    "This crawler is designed to go over this particular prospect dataset only. Thus, it will catalogue tables only for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_name = client_id.lower()+'-raw-crawler_'+received_date\n",
    "glue_client.create_crawler(\n",
    "    Name=crawler_name,\n",
    "    Role='UnderwritingServiceRole',\n",
    "    DatabaseName=database.lower(),\n",
    "    Description='Automated Single Usage - Delete after use',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {'Path': s3_path}\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running Crawler\n",
    "Running the crawler will map the data in the selected s3 bucket into catalogue tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_crawler(Name=crawler_name)\n",
    "time.sleep(60)\n",
    "print(\"It should be stopping now!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Delete Ad-Hoc Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_crawler(Name=crawler_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with Athena Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_client = boto3.client('athena', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating claim data query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_client.create_named_query(\n",
    "    Name='ClaimData',\n",
    "    Database=database,\n",
    "    QueryString=\"SELECT * FROM clientdata\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
