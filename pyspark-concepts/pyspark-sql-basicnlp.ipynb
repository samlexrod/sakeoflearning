{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import webbrowser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import split, explode, regexp_replace, lower, regexp_extract\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id as identity, col, length\n",
    "from pyspark.sql.functions import lag, lead, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkNlp').getOrCreate()\n",
    "webbrowser.open('http://localhost:4040')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Twitter Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Twitter Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive and negative documents\n",
    "pos_tw = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tw = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "\n",
    "# joining documents\n",
    "document = [pos_tw] + [neg_tw]\n",
    "\n",
    "# list to dataframe\n",
    "df = pd.DataFrame(document[0]).append(pd.DataFrame(document[1])).rename(columns={0:'text', 1:'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Resilient Distributed Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "df_rdd = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a job\n",
    "df_rdd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp Table Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView('SqlNlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select('text').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Sentence Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('label').orderBy('text')\n",
    "df_rdd = df_rdd.withColumn('sentence_id', row_number().over(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Invalid Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_rdd.withColumn('clean_text', lower(regexp_replace('text', '[^a-zA-Z#@ ]', '')))\n",
    "df_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = df_clean.select(split('clean_text', ' ').alias('words'), 'label', 'sentence_id')\n",
    "df_split.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Hash and User Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cotains_hash_user = udf(lambda row: any([any([i in x for i in ['#', '@']]) for x in row]))\n",
    "udf_contains_hash_only = udf(lambda row: any(['#' in x for x in row]))\n",
    "udf_clear_hash = udf(lambda row: [x for x in row if '#' not in x], ArrayType(StringType(), True))\n",
    "udf_clear_user = udf(lambda row: [x for x in row if '@' not in x], ArrayType(StringType(), True))\n",
    "\n",
    "df_split = df_split\\\n",
    "    .withColumn('contain_tags', udf_cotains_hash_user('words'))\\\n",
    "    .withColumn('contain_hash_only', udf_contains_hash_only('words'))\\\n",
    "    .withColumn('words_clean', udf_clear_user(udf_clear_hash('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding the Words into Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df_split.select(explode('words_clean').alias('word'), 'label', 'contain_tags', 'contain_hash_only', 'sentence_id')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache in memory to avoid lazy evaluation later\n",
    "df_exp.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df_exp.filter(col('word')!='')\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df_exp.withColumn('id', identity())\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.createOrReplaceTempView('WindowTutorial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id,\n",
    "    LAG(word, 1) OVER(ORDER BY id) AS w1,\n",
    "    word,\n",
    "    LEAD(word, 1) OVER(ORDER BY id) AS w2\n",
    "FROM WindowTutorial\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.orderBy('id')\n",
    "df_exp.select(\n",
    "    'id',\n",
    "    lag('word', 1).over(w).alias('w1'),\n",
    "    'word',\n",
    "    lead('word', 1).over(w).alias('w2')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Window as Subquery: Most common 3-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "GROUP BY label, w1, w2, w3, w4\n",
    "ORDER BY COUNT(1) DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3, w4\n",
    "FROM (\n",
    "    SELECT\n",
    "        word AS w1,\n",
    "        LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "        LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "        LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "    FROM WindowTutorial\n",
    ")\n",
    "ORDER BY w1 DESC, w2, w3, w4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "WITH subquery_cte AS (\n",
    "    SELECT label, w1, w2, w3, w4, COUNT(1) AS phrase_count \n",
    "    FROM (\n",
    "        SELECT\n",
    "            label,\n",
    "            word AS w1,\n",
    "            LEAD(word, 1) OVER(ORDER BY id) AS w2,\n",
    "            LEAD(word, 2) OVER(ORDER BY id) AS w3,\n",
    "            LEAD(word, 3) OVER(ORDER BY id) AS w4\n",
    "        FROM WindowTutorial\n",
    "    )\n",
    "    GROUP BY label, w1, w2, w3, w4\n",
    ")\n",
    "SELECT label, w1, w2, w3, w4, phrase_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        label,\n",
    "        ROW_NUMBER() OVER(PARTITION BY label ORDER BY phrase_count DESC) AS row,\n",
    "        w1, w2, w3, w4, phrase_count\n",
    "    FROM subquery_cte\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY label ASC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp.filter(col('word')=='httptcorcvcyyoiq').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "df_exp = df_exp.filter(~col('word').isin(stopset))\n",
    "df_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
